---
title: "Introduction to SC19084"
author: "Sherry J"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to SC19084}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Overview

__SC19084__ is a simple R package developed to compare the performance of two methods under circumstance of high dimensional regression. Two functions are considered, namely, _ol1_ (min_beta  (2 Lam)^(-1) (z - beta)^2 + p_lambda(|beta|) for L1 penalty) and _uhard_ (min_beta  (2 Lam)^(-1) (z - beta)^2 + lambda0 |beta| + p_lambda(|beta|) for hard-thresholding penalty). 

For ol1 function, we are able to estimate coefficient vector beta for L1 penalty and the uhard function was constructed to implement the high dimensional threshold regression.

##ol1 function

To use this function, error and design matrix are required to be generated according to some rules.

```{r,eval=TRUE}
library(SC19084)
set.seed(123)
n<-10;p<-15
b0<-matrix(0,p,1)
b0[1:7]<-c(1,-0.5,0.7,-1.2,-0.9,0.3,0.55)
epsi <-0.01
cormat=matrix(1,p,p)
corval = 0.5
for (i in 1:(p-1)){
  for (j in (i+1):p){
    cormat[i,j]=corval^abs(i-j)
    cormat[j,i]=cormat[i,j]
 }
}
sigmat=cormat^0.5
X<-matrix(rnorm(n*p),n,p)%*%sigmat
y<-X%*%b0+0.4*matrix(rnorm(n),n,1)
Xsca <- sqrt(colSums(X^2))/sqrt(n)
X <- X/(matrix(1,n,1)%*%Xsca)
```

Then get the input parameters from what we generated above.

```{r}
XXmat <- n^(-1)*t(X)%*%X
cvec <- n^(-1)*t(X)%*%y
lammax = max(abs(cvec))
lammin = epsi*lammax
lamvec = lammax - (lammax - lammin)/(50 - 1)*c(0:(50 - 1))
inival<-rep(0,p)
lambda<-lamvec[50]
varset<-vector(mode="numeric")
```

Obtain coefficient vector beta for L1 penalty regression.

```{r}
ol1(XXmat, cvec, inival, lambda, varset, 50, 1e-4)
```

##uhard function

As one step to realise ICA algorithm which is applyed to implement the  high dimensional threshold regression, error and design matrix are also required to be generated.

```{r}
set.seed(321)
n<-10;p<-15
b0<-matrix(0,p,1)
b0[1:7]<-c(1,-0.5,0.7,-1.2,-0.9,0.3,0.55)
epsi <-0.01
cormat=matrix(1,p,p)
corval = 0.5
for (i in 1:(p-1)){
  for (j in (i+1):p){
    cormat[i,j]=corval^abs(i-j)
    cormat[j,i]=cormat[i,j]
 }
}
sigmat=cormat^0.5
X<-matrix(rnorm(n*p),n,p)%*%sigmat
y<-X%*%b0+0.4*matrix(rnorm(n),n,1)
Xsca <- sqrt(colSums(X^2))/sqrt(n)
X <- X/(matrix(1,n,1)%*%Xsca)
```

Then get the input parameters from what we generated above.

```{r}
XXmat <- n^(-1)*t(X)%*%X
z1 = XXmat[1, 1]
Lam<-1/z1
cvec <- n^(-1)*t(X)%*%y
z<-cvec[1]/z1
lammax = max(abs(cvec))
lammin = epsi*lammax
lamvec = lammax - (lammax - lammin)/(50 - 1)*c(0:(50 - 1))
lambda0<-lamvec[49]
lambda<-lamvec[50]
```

Obtain one number of coefficient vector beta.

```{r}
uhard(z, Lam, lambda0, lambda)
```

Through this method, coefficient vector beta for hard-thresholding penalty regression shall be obtained.

##Question

Use knitr to produce at least 3 examples (texts, figures,
tables).

##Answer

Example 1
```{r}
#par(bg="ivory3")
set.seed(123)
x=rnorm(100,mean=100,sd=10)
hist(x,breaks=20,col=2,main="Simple Histogram")
```

A rough situation of Gaussian distribution can be observed.



Example 2
```{r}
A=head(freeny)
M=A[-2]
knitr::kable(M)
```

Example 3
```{r}
x=rbinom(50,3000,0.001)
y=rpois(50,3)
lm.cf=lm(x~y)
summary(lm.cf)$coef
```

The $R^2$ is `r summary(lm.cf)$r.squared`

It shows the approximation between the poission distribution and the binomial distribution

##Question 3.4

The Rayleigh density [156, Ch. 18] is
$$
f(x)=\frac{x}{\sigma^{2}} e^{-x^{2} /\left(2 \sigma^{2}\right)}, \quad x \geq 0, \sigma>0
$$
Develop an algorithm to generate random samples from a Rayleigh(??) distribution. Generate Rayleigh(??) samples for several choices of ?? > 0 and check that the mode of the generated samples is close to the theoretical mode ??(check the histogram).

##Answer for 3.4

First, we use the inverse transform method to generate random number with the distribution in the title.

Rayleigh distribution depend on the choice of ?? so that a function is developed as below. Attention, we can also check that the mode of the generated samples is close to the theoretical mode in the function.

```{r}
n<-1e4
myrn<-function(sigma){
u<-runif(n)
x<-(-2*(sigma^2)*log(1-u))^(1/2)
hist(x,breaks=15, prob = TRUE,#main=expression(f(x)==x/sigma^2*exp(-(x^2)/(2*sigma^2))))
      main=paste("sigma=",sigma))
a <- seq(.001, 10, .001);y<-a/(sigma^2)*exp(-(a^2)/(2*sigma^2))
lines(a, y)
}
```

Then we generate Rayleigh samples for several choices of ?? and check that the mode of the generated samples is close to the theoretical mode by checking the histogram.

```{r}
#par(mfrow=c(2,2))
myrn(.5);myrn(1);myrn(2);myrn(4)
```

We can observe that the mode of the generated samples is close to the theoretical mode where $\sigma$=0.5,1,2,4.  

#Wonder

Is there any other intuitive method to check that the mode of the generated samples is close to the theoretical mode ?

##Question 3.11

Generate a random sample of size 1000 from a normal location mixture. The components of the mixture have $N(0,1)$ and $N(3,1)$ distributions with mixing probabilities $p_{1}$ and $p_{2}=1-p_{1}$ . Graph the histogram of the sample with density superimposend, for $p_{1}$ = 0.75. Repeat with different values for $p_{1}$ and observe whether the empirical distribution of the mixture appears to be bimodal. Make a conjecture about the values of $p_{1}$ that produce bimodal mixtures.

##Answer for 3.11

Suppose $X_{1} \sim N(0,1)$ and $X_{2} \sim N(3,1)$ are independent.The notation $Z=p_{1} X_{1}+p_{2} X_{2}$ denotes the mixture of $X_{1}$ and $X_{2}$ .

To simulate the mixture:

1. Generate $x_{1} \sim N(0,1)$ and $x_{2} \sim N(3,1)$

2. Deliver $z=p_{1} x_{1}+p_{2} x_{2}$ for different values of $p_{1}$

```{r}
n <- 1e3
x1 <- rnorm(n,0,1)
x2 <- rnorm(n,3,1)
myz<-function(r){  # function to generate different z as value of p1 changes
z <- r*x1+(1-r)*x2
hist(z,breaks=30,main=paste("z=",r,"X1+",(1-r),"X2"))
}
#par(mfrow=c(2,2))
myz(.75);myz(.5);myz(.25);myz(.1)#show histograms where p1=0,75,0.5,0.25,0.1
```

From figures above, we can observe the empirical distribution of the mixture appears to be unimodal when the value of $p_{1}$ take o.75,0.25 and 0.1, while appears to be bimodal for $p_{1}$=0.5.

Thus, we Make a conjecture that the value of $p_{1}$ that produce bimodal
mixture is 0.5.

#Wonder

What are the factors influence the distance of two peaks?



##Question 3.18

Write a function to generate a random sample from a $W_{d}(\Sigma, n)$ (Wishart)
distribution for $n>d+1 \geq 1$, based on Bartlett??s decomposition.

##Answer for 3.18

To generate a random sample from a $W_{d}(\Sigma, n)$ (Wishart)
distribution based on Bartlett??s decomposition, we set $\Sigma$ a symmetric and positive $d \times d$ matrix as below
$$
\Sigma=\left(\begin{array}{ccc}{9} & {18} & {3} \\ {18} & {40} & {16} \\ {3} & {16} & {27}\end{array}\right)
$$
for a $W_{3}(\Sigma, n)$ (Wishart) distribution.

$T=\left(T_{i j}\right)$ is a lower triangular
$d \times d$ random matrix with independent entries satisfying

$$
\begin{array}{l}{\text { 1. } T_{i j} \stackrel{i i d}{\sim} N(0,1), i>j} \\ {\text { 2. } T_{i i} \sim \sqrt{\chi^{2}(n-i+1)}, i=1, \ldots, d}\end{array}
$$

According to Bartlett??s decomposition, $A=T T^{T}$ has a $W_{3}\left(I_{3}, n\right)$ distribution. Also, we obtain the Choleski factorization $\Sigma=L L^{T}$ where $L$ is lower triangular.
Finally we get $X=L A L^{T}$

On the basis of what we discussed above, a function is written to generate a random sample from a $W_{3}(\Sigma, n)$ (Wishart)
distribution for any $n$ satisfying $n>d+1 \geq 1$.

```{r}
myW=function(n){
  sigma<-matrix(c(9,18,3,18,40,16,2,16,27),3,3)#set a three dimentional scale matrix sigma
  a<-rep(0,9);T<-matrix(a,3,3)
  T[1,1]<-sqrt(rchisq(1,n));T[2,1]<-rnorm(1);T[2,2]<-sqrt(rchisq(1,n-1))
  T[3,1]<-rnorm(1);T[3,2]<-rnorm(1);T[3,3]<-sqrt(rchisq(1,n-2))#set matrix T
  A<-T%*%t(T) #obtain matrix A
  L=matrix(c(3,6,1,0,2,5,0,0,1),3,3)#show the Choleski factorization of sigma
  X<-L%*%A%*%t(L) # obtain random sample X
  X
}
```

Now we generate a random sample from a $W_{3}(\Sigma, 30)$ (Wishart) distribution.

```{r}
myW(30)
```

##Question **5.1**

 Compute a Monte Carlo estimate of
$$
\int_{0}^{\pi / 3} \sin t d t
$$

 and compare your estimate with the exact value of the integral.

##Answer for **5.1**

* ###Basic idea

 Represent the integral as the expected value of a function of a uniform random variable..

$$
\theta=\int_{0}^{\pi / 3} \sin t d t=\int_{0}^{\pi / 3} \frac{1}{\pi / 3} \cdot \frac{\pi}{3} \sin t d t=\frac{\pi}{3} E(\sin X) \triangleq E(g(X)) \quad X \sim U\left(0, \frac{\pi}{3}\right)
$$

 1. Generate $X_{1}$ ,...,$X_{m}$, iid from Uniform$\left(0, \frac{\pi}{3}\right)$.
 
 2. Compute $\overline{g(X)}=\frac{1}{m} \sum_{i=1}^{m} g\left(X_{i}\right)$.
 
 3. $\hat{\theta}=(\frac{\pi}{3}-0) \overline{g(X)}$.

 We can certainly handle this problem with analytic method, saying calculus. Thus we will compare the estimate with the exact value of the integral.

* ###Code

```{r}
set.seed(12345)   
# for reproducible research 
m <-1e5
x <-runif(m,0,pi/3)
# X with uniform distribution
g <-(pi/3)*sin(x)
est1 <-mean(g)
# Monte Carlo estimate of the integral
print(c(est1,cos(0)-cos(pi/3)))
# compare with the exact value
```

 It can be observed that our estimate is correct to three decimal places.

 More reliably, the standard error of $\hat{\theta}=\frac{1}{\mathrm{m}} \sum_{\mathrm{i}=1}^{\mathrm{m}} \mathrm{g}\left(\mathrm{x}_{\mathrm{i}}\right)$ and error bounds are supposed to be given.

```{r}
d <-sd(g)/sqrt(m)
# standard error of Monte Carlo estimate
c(est1 - 1.96 * d, est1 + 1.96 * d)
# 95% confidence intervals
```



##Question **5.10**

 Use Monte Carlo integration with antithetic variables to estimate
$$
\int_{0}^{1} \frac{e^{-x}}{1+x^{2}} d x
$$
 and find the approximate reduction in variance as a percentage of the variance
without variance reduction.

##Answer for **5.10**

* ###Basic idea

 1. Write a function to compute Monte Carlo estimate the same way as Q5.1.
 
 2. Write a function to estimate the integral using Monte Carlo integration with antithetic variables.
 
 3. Compare the two methods in terms of variance.
 

* ###Code

```{r}
MC <-function(n){
  x <-runif(n)
  g <-exp(-x)/(1+x^2)
  est101 <-mean(g)
  est101
}
# function of Monte Carlo estimate without antithetic variables
MC.anti<-function(n){
  u <-runif(n/2)
  v <-1-u
  g1 <-exp(-u)/(1+u^2)
  g2 <-exp(-v)/(1+v^2)
# antithetic variables
  est102 <-mean(g1+g2)/2
  est102
}
# function of Monte Carlo estimate with antithetic variables
m <-1000
set.seed(321)
MC1 <-MC2 <-numeric(m)
for (i in 1:m){
  MC1[i] <-MC(m)
  MC2[i] <-MC.anti(m/2)
}
```
A simulation under two methods, the simple Monte Carlo integration approach and the antithetic variable approach, has been done. Then we can estimate the approximate reduction in variance as follow.

```{r}
print((var(MC1) - var(MC2))/var(MC1))
```

The antithetic variable approach achieved approximately 92.98% reduction in variance.


##Question **5.15**

Obtain the stratified importance sampling estimate in Example 5.13 and compare it with the result of Example 5.10

##Answer for **5.15**

* ###Basic idea

1. Devide the interval $(0,1)$ into five intervals corresponding to equal areas 1/5 under the density $f(x)$ (impartance function $f_{3}(x)$ in expample 5.10) as below.

$I_{1}=(0,0.135)$?? $I_{2}=(0.135,0.291)$?? $I_{3}=(0.291,0.477)$?? $I_{4}=(0.477,0.705)$?? $I_{5}=(0.705,1)$ 

2. Define
$$
g_{j}(x)=\left\{\begin{array}{cc}{\frac{e^{-x}}{1+x^{2}}} & {, x \in I_{j}} \\ {0} & {, \text { else }}\end{array}\right.
$$

On the $j^{t h}$ subinterval variables are generated from the density
$$
f_{j}(x)=\frac{5 e^{-x}}{1-e^{-1}} \quad x \in I_{j}
$$

For each j = 1,...,5 we simulate an importance sample size r, compute the importance sampling estimator $\hat{\theta}_{j}$ of $\theta_{j}$ on the $j^{t h}$ subinterval, $\hat{\theta}^{S I}= \sum_{j=1}^{k} \hat{\theta}_{j}$

3. We also compute the importance sampling estimator with importance function $f_{3}(x)$ in example 5.10

4. Compare the results of two methods.

* ###Code

```{r}
cdf <-function(p){
  uniroot(function(x) exp(1)*(1-exp(-x))-p*(exp(1)-1),c(0,1),tol=1e-9)$root
}
# funtion to compute quantiles
x <-numeric(6)
p <-rep(.2,5)
cp <-cumsum(p)
for(i in 1:5){
  x[i+1]<-cdf(cp[i])
}
# five subintervals
m <-1e4
# number of replicates
k <-5
# number of strata
r <- m/k
# replicates per stratum
n <- 50 
# number of times to repeat the estimation
T <- numeric(k)
estimates <- matrix(0,n,2)
g <- function(x){
  exp(-x - log(1+x^2)) * (x > 0) * (x < 1)
}
F <- function(x){
  exp(-x)*5/(exp(-1)-1) * (x > 0) * (x < 1)
}
for(i in 1:n) {
  v<- runif(m)
  # inverse transform method
  s <- - log(1 - v * (1 - exp(-1)))
  # random number generated from f3(x) in example 5.10
  estimates[i, 1] <- mean(g(s) / (exp(-s) / (1 - exp(-1))))
  # MC estimator with importance sampling method
for (j in 1:k){
  u<-runif(r)
  # inverse transform method
  #t<-uniroot(function(y) F(y)-F(x[j])-u,c(0,1),tol=1e-9)$root
  t<- -log(exp(-x[j])-u*(1-exp(-1))/5)
  # random number generated from f(x)
  gi<-g(t)
  fi<-exp(-t)*5/(1-exp(-1))
  Y <- gi/fi
  T[j]<-mean(Y)
  # MC estimator with importance sampling method in jth interval
}
  estimates[i, 2] <- sum(T)
  # stratified importance sampling estimate
}
```

compute mean of r simulations of both methods.

```{r}
 apply(estimates, 2, mean)
```

we can observe the mean of two methods is identical to three decimal places.

```{r}
 apply(estimates, 2, var)
```

the variance tells stratified importance sampling can do much better than simple importance sampling.

* ###Wonder

Is there any connection between the number of subinterval and the variance of our results?

####Conjection verification

```{r}
xt <-numeric(11)
pt <-rep(.1,10)
cpt <-cumsum(pt)
for(i in 1:10){
  xt[i+1]<-cdf(cpt[i])
}
kt <-10
# number of strata
rt <- m/kt
# replicates per stratum
F <- numeric(kt)
test <- matrix(0,n,1)
for(i in 1:n) {
  for (j in 1:kt){
    ut<-runif(rt)
    tt<- -log(exp(-xt[j])-ut*(1-exp(-1))/10)
    git<-g(tt)
    fit<-exp(-tt)*10/(1-exp(-1))
    Yt <- git/fit
    F[j]<-mean(Yt)
  }
  # MC estimator with importance sampling method in jth interval
  test[i] <- sum(F)
  # stratified importance sampling estimate
}
```

```{r}
 mean(test)
```

```{r}
 var(test)
```
Slight improvement can be observed when we apply narrower partition.

##Question **6.5**

Suppose a $95 \%$ symmetric t-interval is applied to estimate a mean, but the sample data are non-normal. Then the probability that the confidence interval covers the mean is not necessarily equal to 0.95. Use a Monte Carlo experiment to estimate the coverage probability of the t-interval for random samples of $\chi^{2}(2)$ data with sample size n = 20. Compare your t-interval results with the simulation results in Example 6.4. (The t-interval should be more robust to departures from normality than the interval for variance.)

##Answer for **6.5**

* ###Basic idea

If $X_{1}, \ldots, X_{n}$ is a random sample from a Normal$\left(\mu, \sigma^{2}\right)$distribution,$n \geq 2$, then
$$
\frac{\bar{X}-E(\bar{X})}{\sqrt{\operatorname{Var}(\bar{X})}} \sim t(n-1)
$$

Though $X \sim \chi^{2}(2)$ now, we also use the above type to estimate mean and the upper confidence limit is
$$
\bar{X}+s d(\bar{X}) \cdot t_{1-\alpha}(n-1)
$$

####Solution processes

1. Set random sample size $n$ and confidence coefficient $\alpha$ (given in the item content)

2. Set simulation time $m$

3. Generate the $j^{t h}$ ($j=1, \ldots, m$) random sample $X_{1}^{(j)}, \ldots, X_{n}^{(j)}$

4. Compute $y_{j}=I\left(\bar{X} \in C_{j}\right)$ for the $j^{t h}$ sample and teh empirical confidence level $\bar{y}=\frac{1}{m} \sum_{j=1}^{m} y_{j}$


* ###Code

```{r}
rm(list=ls())
# remove variables
set.seed(123)
# for repruducible research
n<-20;alpha<-.05
CI_1<-function(a,b){
  x<-rchisq(a,2)
  UCL<-mean(x)+sqrt(var(x)/a)*qt(1-b,df=a-1)
  return(UCL)
}
# function to obtain confidence interval appling t-interval
CI_2<-function(c,d){
  y <- rnorm(c, mean=0, sd=2)
  UCL<-(c-1) * var(y) / qchisq(d, df=c-1)
  return(UCL)
}
# function to obtain confidence interval in example 6.4
```

We've got functions to obtain two sorts of confidence interval respectively, now let's compare the results through some setted values.
 

```{r results='asis'}
m<-1e4
# number of replication
ci_1<-replicate(m,expr=CI_1(n,alpha))
# upper confidence limit appling t-interval
ci_2<-replicate(m,expr=CI_2(n,alpha))
# upper confidence limit in example 6.4
c<-matrix(c(mean(ci_1>2),mean(ci_2>4)),1,2)
#Monte Carlo estimates of confidence level
colnames(c)<-c("ECL_1","ECL_2")
pander::pandoc.table(c)
```

We can observe that only 89.37% of the intervals contained the population mean, which is far from the 95% coverage under normality

* ###Wonder

Is there any method which can improve the coverage probability?

```{r results='asis'}
set.seed(002)
n_2<-40;n_3<-80;n_4<-160
# a series of sample size
ci_11<-replicate(m,expr=CI_1(n,alpha))
ci_12<-replicate(m,expr=CI_1(n_2,alpha))
ci_13<-replicate(m,expr=CI_1(n_3,alpha))
ci_14<-replicate(m,expr=CI_1(n_4,alpha))
d<-matrix(c(mean(ci_11>2),mean(ci_12>2),mean(ci_13>2),mean(ci_14>2)),1,4)
#Monte Carlo estimates of confidence level
colnames(d)<-c("n=20","n=40","n=80","n=160")
pander::pandoc.table(d)
```

As clearly observed, the coverage probability improves with increasing sample size.

It's suggestive that an appropriate increase of sample size can work to improve coverage probability. 

##Question **6.6**

Estimate the 0.025, 0.05, 0.95, and 0.975 quantiles of the skewness $\sqrt{b_{1}}$ under normality by a Monte Carlo experiment. Compute the standard error of the estimates from (2.14) using the normal approximation for the density (with exact variance formula). Compare the estimated quantiles with the quantiles of the large sample approximation $\sqrt{b_{1}} \approx N(0,6 / n)$.

##Answer for **6.6**

* ###Basic idea

The skewness $\sqrt{b_{1}}$ is
$$
\sqrt{b_{1}}=\frac{\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{3}}{\left(\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}\right)^{3 / 2}}
$$

 If $X \sim N(0,1)$, then $\sqrt{b_{1}} \approx N(0,6 / n)$
 
 Obviously, we can generate random numbers under normality to get samples of the skewness, then estimate quantiles of it by a Monte Carlo experiment.
 
 
####Solution processes

1. Set random sample size $n$ and simulation time $m$ 

2. Generate the $j^{t h}$ ($j=1, \ldots, m$) random sample $X_{1}^{(j)}, \ldots, X_{n}^{(j)}$ under $N(0,1)$

3. Compute the skewness of sample $\sqrt{b_{1}}^{(j)}$ 

4. Estimate quantiles of the skewness by a Monte Carlo experiment 

5. Obtain quantiles of variables under $N(0,6 / n)$

6. Compare the two sorts of quantiles

7. Compute the standard error of the estimates from (2.14) using the normal approximation for the density
 

* ###Code

```{r results='asis'}
rm(list=ls())
# remove variables
set.seed(0321)
n<-1e3
# sample size
m<-1e4
# number of replication
sk <- function(x) {
#computes the sample skewness.
xbar <- mean(x)
m3 <- mean((x - xbar)^3)
m2 <- mean((x - xbar)^2)
return( m3 / m2^1.5 )
}
q1<-quantile(replicate(m,expr=sk(rnorm(n))),c(.025,.05,.95,.975))
# quantiles of the skewness under normality by a Monte Carlo experiment
q2<-qnorm(c(.025,.05,.95,.975),mean=0,sd=sqrt(6/n))
# quantiles of the large sample approximation
x<-matrix(c(q1,q2),2,4,byrow=T)
rownames(x)<-c("eatimate","approximation")
colnames(x)<-c("0.025","0.05","0.95","0.975")
pander::pandoc.table(x)
```

As is shown in the table, the estimated quantiles are close to the quantiles under $N(0,6 / n)$.

Then compute the standard error of the estimates.

```{r results='asis'}
set.seed(007)
sd<-numeric(4)
c<-c(.025,.05,.95,.975)
for (i in 1:4){
  sd[i]<-sqrt(c[i]*(1-c[i])/(n*(dnorm(x[1,i],mean=0,sd=sqrt(6*(n-2)/((n+1)*(n+3))))^2)))
}
# standard error of the estimates
y<-matrix(sd,1,4)
rownames(y)<-"standard error"
colnames(y)<-c("0.025","0.05","0.95","0.975")
pander::pandoc.table(y)
```

* ###Wonder

Decrease the sample size and see what will happen to standard error.

```{r results='asis'}
n_2<-1e2
# decreased sample size
sd_2<-numeric(4)
for (i in 1:4){
  sd_2[i]<-sqrt(c[i]*(1-c[i])/(n_2*(dnorm(x[1,i],mean=0,sd=sqrt(6*(n_2-2)/((n_2+1)*(n_2+3))))^2)))
}
z<-matrix(sd_2,1,4)
rownames(z)<-"standard error 2"
colnames(z)<-c("0.025","0.05","0.95","0.975")
pander::pandoc.table(z)
```

We can observe the standard error increases when sample size decreases, but more experiments are needed to draw a conclusion.

##Question **7.6**

Efron and Tibshirani discuss the scor (bootstrap)test score data on 88 students who took examinations in five subjects. The first two tests (mechanics, vectors) were closed book and the last three tests (algebra, analysis, statistics) were open book. Each row of the data frame is a set of scores $\left(x_{i 1}, \dots, x_{i 5}\right)$ for the $i^{t h}$ student. Use a panel display to display the scatter plots for each pair of test scores. Compare the plot with the sample correlation matrix. Obtain bootstrap estimates of the standard errors for each of the following estimates: $\hat{\rho}_{12}=\hat{\rho}(\mathrm{mec}, \mathrm{vec})$,$\hat{\rho}_{34}=\hat{\rho}(\mathrm{alg}, \mathrm{ana})$,$\hat{\rho}_{35}=\hat{\rho}(\mathrm{alg}, \mathrm{sta})$, $\hat{\rho}_{45}=\hat{\rho}(\text { ana, sta })$.

##Answer for **7.6**

* ###Basic idea

For using a panel display to display the scatter plots for each pair of test scores and comparing the plot with the sample correlation matrix, we found the $pairs$ function convenient to meet our requirements.


* ###Flowchart

1. Display the scatter plots for ten pairs of test scores with $pairs$ function.

2. Show the correlation coefficient of each pair of variables.(panel functions are taken from the $pairs$ help page)

3. Compare the scatter plots with the sample correlations.

4. Write a function that retures $b^{t h}$ replicate from $b^{t h}$ bootstrap sample, which denotes the sample correlation.

5. Using the $boot$ function to obtain bootstrap estimates of the standard errors for the four estimates respectively.

* ###Code

```{r}
rm(list=ls())
# remove variables
#library(ggplot2)
# to display scatter plots
library(bootstrap)
# to obtain data
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  r <- abs(cor(x, y))
  txt <- format(c(r, 0.123456789), digits = digits)[1]
  txt <- paste0(prefix, txt)
  if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
  text(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)
}
# put (absolute) correlations on the upper panels with size proportional to the correlations
pairs(scor,upper.panel = panel.cor,gap=0, row1attop=FALSE)
```

As shown in the panel, the pair of scores with higher correlation has a more concentrated scatter distribution.

Then compute bootstrap estimates of the standard errors for the estimates in question.

```{r results='asis'}
B <- 1e3
set.seed(1234)
r12 <- function(x, i) {
#want correlation of columns 1 and 2
cor(x[i,1], x[i,2])
}
r34 <- function(x, i) {
#want correlation of columns 3 and 4
cor(x[i,3], x[i,4])
}
r35 <- function(x, i) {
#want correlation of columns 3 and 5
cor(x[i,3], x[i,5])
}
r45 <- function(x, i) {
#want correlation of columns 4 and 5
cor(x[i,4], x[i,5])
}
library(boot) 
# for boot function
sd_boot<-numeric(4)
#for (i in 1:4){
sd_boot[1]<-sd(boot(data=scor,statistic=r12,R=2000)$t)
sd_boot[2]<-sd(boot(data=scor,statistic=r34,R=2000)$t)
sd_boot[3]<-sd(boot(data=scor,statistic=r35,R=2000)$t)
sd_boot[4]<-sd(boot(data=scor,statistic=r45,R=2000)$t)
#}
a<-matrix(sd_boot,1,4)
colnames(a)<-c("Cor(1,2)","Cor(3,4)","Cor(3,5)","Cor(4,5)")
rownames(a)<-"sd.boot"
pander::pandoc.table(a)
```



##Question **7.B**

Conduct a Monte Carlo study to estimate the coverage probabilities of the standard normal bootstrap confidence interval, the basic bootstrap confidence interval, and the percentile confidence interval. Sample from normal populations (skewness 0) and $\chi^{2}(5)$ distributions (positive skewness) and compare the empirical coverage rates for the sample skewness statistic. Find the proportion of times that the confidence intervals miss on the left, and the porportion of times that the confidence intervals miss on the right.

##Answer for **7.B**

* ###Basic idea
There are several approaches to obtaining approximate confidence intervals for the sample skewness in a bootstrap. The methods include the standard normal bootstrap confidence interval, the basic bootstrap confidence interval and the bootstrap percentile confidence interval.

* ###Flowchart

1. Choose sample size n for the skewness.

2. Choose sample size m for bootstrap confidence interval.

3. Choose number of replicate for empirical coverage rates.

4. Run R function boot.ci{boot} implements the three bootstrap CI methods.

5. Conduct a Monte Carlo study to estimate the coverage probabilities and the proportion of times that the CI miss on the left along with on the right for the three bootstrap CI methods with normal population and $\chi^{2}(5)$ distributions respectively.

* ###Code

```{r results='asis'}
rm(list=ls())
# remove variables
library(boot)
set.seed(0321)
sk_nor<-0;sk_chi<-2/sqrt(5/2)
# skewnesses with normal distribution and chi-square distribution
n <- 20
# sample size for statistic
m<-10
# sample size for confidence interval
M<-1e2
# number of replicate for empirical coverage rates
sk.boot <- function(x,i) {
  # function to compute the sample skewness statistic
  xbar <- mean(x[i,])
  m3 <- mean((x[i,] - xbar)^3)
  m2 <- mean((x[i,] - xbar)^2)
  return( m3 / m2^1.5 )
}
ci.norm_nor<-ci.norm_chi<-ci.basic_nor<-ci.basic_chi<-ci.perc_nor<-ci.perc_chi<-matrix(NA,M,2)
for(i in 1:M){
  x<-matrix(rnorm(m*n),m,n)
  # sample from normal population
  y<-matrix(rchisq(m*n,5),m,n)
  # sample from chi-square population
  de_nor <- boot(x, statistic = sk.boot, R = 1000)
  ci_nor <- boot.ci(de_nor,type=c("norm","basic","perc"))
  # bootstrap CI for normality
  de_chi <- boot(y, statistic = sk.boot, R = 1000)
  ci_chi <- boot.ci(de_chi,type=c("norm","basic","perc"))
  # bootstrap CI for chi-square
  ci.norm_nor[i,]<-ci_nor$norm[2:3]
  ci.basic_nor[i,]<-ci_nor$basic[4:5]
  ci.perc_nor[i,]<-ci_nor$percent[4:5]
  # extract three sorts of CI for normality
  ci.norm_chi[i,]<-ci_chi$norm[2:3]
  ci.basic_chi[i,]<-ci_chi$basic[4:5]
  ci.perc_chi[i,]<-ci_chi$percent[4:5]
  # extract three sorts of CI for chi-square
}
CR_nor <- c(mean(ci.norm_nor[,1]<=sk_nor & ci.norm_nor[,2]>=sk_nor),mean(ci.basic_nor[,1]<=sk_nor & ci.basic_nor[,2]>=sk_nor),mean(ci.perc_nor[,1]<=sk_nor & ci.perc_nor[,2]>=sk_nor),mean(ci.norm_nor[,1]>=sk_nor ),mean(ci.basic_nor[,1]>=sk_nor),mean(ci.perc_nor[,1]>=sk_nor),mean(ci.norm_nor[,2]<=sk_nor ),mean(ci.basic_nor[,2]<=sk_nor),mean(ci.perc_nor[,2]<=sk_nor))
# Monte Carlo estimates of CI and miss rates of both sides for normality
b<-matrix(CR_nor,3,3,byrow=T)
colnames(b)<-c("norm","basic","perc")
rownames(b)<-c("CR for normality","miss rates on the left","miss rates on the right")

CR_chi <- c(mean(ci.norm_chi[,1]<=sk_chi & ci.norm_chi[,2]>=sk_chi),mean(ci.basic_chi[,1]<=sk_chi & ci.basic_chi[,2]>=sk_chi),mean(ci.perc_chi[,1]<=sk_chi & ci.perc_chi[,2]>=sk_chi),mean(ci.norm_chi[,1]>=sk_chi ),mean(ci.basic_chi[,1]>=sk_chi),mean(ci.perc_chi[,1]>=sk_chi),mean(ci.norm_chi[,2]<=sk_chi ),mean(ci.basic_chi[,2]<=sk_chi),mean(ci.perc_chi[,2]<=sk_chi))
#Monte Carlo estimates of CI and miss rates of both sides for chi-square
c<-matrix(CR_chi,3,3,byrow=T)
colnames(c)<-c("norm","basic","perc")
rownames(c)<-c("CR for chi-square","miss rates on the left","miss rates on the right")

pander::pandoc.table(b)
pander::pandoc.table(c)
```

We can observe that for normal populations, the coverage rates for the sample skewness are higher than for $\chi^{2}(5)$ distributions in all the three kinds of confidence interval.

At the same time, for normality the miss rates on the left are close to that on the right, while for $\chi^{2}(5)$ distributions the confidence interval always have higher miss rates on the right than on the left. Assume that's the result of difference of distribution symmetry now let's check our assumption.


```{r results='asis'}
set.seed(123)
sk_exp<-2
# skewnesses with exponential distribution
ci.norm_exp<-ci.perc_exp<-ci.basic_exp<-matrix(NA,M,2)
for(i in 1:M){
  x<-matrix(rexp(m*n,2),m,n)
  # sample from exp(2) population
  de_exp <- boot(x, statistic = sk.boot, R = 1000)
  ci_exp <- boot.ci(de_exp,type=c("norm","basic","perc"))
  # bootstrap CI for exponential distribution
  ci.norm_exp[i,]<-ci_exp$norm[2:3]
  ci.basic_exp[i,]<-ci_exp$basic[4:5]
  ci.perc_exp[i,]<-ci_exp$percent[4:5]
  # extract three sorts of CI for exponential distribution
}
  CR_exp <- c(mean(ci.norm_exp[,1]<=sk_exp & ci.norm_exp[,2]>=sk_exp),mean(ci.basic_exp[,1]<=sk_exp & ci.basic_exp[,2]>=sk_exp),mean(ci.perc_exp[,1]<=sk_exp & ci.perc_exp[,2]>=sk_exp),mean(ci.norm_exp[,1]>=sk_exp ),mean(ci.basic_exp[,1]>=sk_exp),mean(ci.perc_exp[,1]>=sk_exp),mean(ci.norm_exp[,2]<=sk_exp ),mean(ci.basic_exp[,2]<=sk_exp),mean(ci.perc_exp[,2]<=sk_exp))
# Monte Carlo estimates of CI and miss rates of both sides for exponential
d<-matrix(CR_exp,3,3,byrow=T)
colnames(d)<-c("norm","basic","perc")
rownames(d)<-c("CR for exp(2)","miss rates on the left","miss rates on the right")
pander::pandoc.table(d)

```

For exp(2) population, we observed the same situation as $\chi^{2}(5)$ distributions in regard to the significant differentiation between miss rates on the left and on the right.

##Question **6.7**

Estimate the power of the skewness test of normality against symmetric $\operatorname{Beta}(\alpha, \alpha)$ distributions and comment on the results. Are the results different for heavy-tailed symmetric alternatives such as $t(\nu)$?

##Answer for **6.7**

* ###Basic idea

The skewness $\sqrt{\beta_{1}}$ of a random variable $X$ is defined by
$$
\sqrt{\beta_{1}}=\frac{E\left[\left(X-\mu_{X}\right)\right]^{3}}{\sigma_{X}^{3}}
$$
The sample coefficient of skewness is denoted by $\sqrt{b_{1}}$, and defined as
$$
\sqrt{b_{1}}=\frac{\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{3}}{\left(\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}\right)^{3 / 2}}
$$
 If the distribution of $X$ is normal, then $\sqrt{b_{1}}$ is asymptotically normal with mean
0 and variance $\frac{6(n-2)}{(n+1)(n+3)}$. The hypotheses are
$$
H_{0}: \sqrt{\beta_{1}}=0 ; \quad H_{1}: \sqrt{\beta_{1}} \neq 0
$$
First, we estimate by simulation the power of the skewness test of normality against symmetric $\operatorname{Beta}(\alpha, \alpha)$ distributions, which is denoted by $\operatorname{Beta}(\alpha, \alpha)$.

Then comes two sorts of heavy-tailed symmetric alternatives, $t(\nu)$ and $U(0,1)$, compare the results and draw some conclusions.

####Flowchart

1. Set random sample size $n$ and confidence coefficient $\alpha$ 

2. Set simulation time $m$

3. Generate the $j^{t h}$ ($j=1, \ldots, m$) random sample $X_{1}^{(j)}, \ldots, X_{n}^{(j)}$ under the conditions of  symmetric $\operatorname{Beta}(\alpha, \alpha)$ distributions

4. Estimate the power of the skewness test of normality against symmetric $\operatorname{Beta}(\alpha, \alpha)$ distributions

5. Choose two sorts of heavy-tailed symmetric alternatives, $t(\nu)$ and $U(0,a)$, generate random sample under the conditions of the two respectively

6. Estimate the power of the skewness test of normality against the two heavy-tailed symmetric alternatives


* ###Code

```{r}
rm(list=ls())
# remove variables
set.seed(123)
alpha <- .1
n <- 30
m <- 2500
a<-seq(1,20) # parameter of Beta distribution
#epsilon <- c(seq(0, .15, .01), seq(.15, 1, .05))
#N <- length(epsilon)
N <- length(a)
power <- numeric(N)
sk <- function(x) {
  # computes the sample skewness coeff.
  xbar <- mean(x)
  m3 <- mean((x - xbar)^3)
  m2 <- mean((x - xbar)^2)
  return( m3 / m2^1.5 )
}
cv <- qnorm(1-alpha/2, 0, sqrt(6*(n-2) / ((n+1)*(n+3))))
# critical value for the skewness test
for (j in 1:N) { #for each parameter of Beta distribution
e <- a[j]
stests <- numeric(m)
for (i in 1:m) { #for each replicate
  x<- rbeta(n,e,e)
  stests[i] <- as.integer(abs(sk(x)) >= cv)
}
power[j] <- mean(stests)
}
#plot power vs parameter of Beta distribution
plot(a, power, type = "b", xlab = bquote(alpha), ylim = c(0,1))
abline(h = .1, lty = 3)
se <- sqrt(power * (1-power) / m) #add standard errors
lines(a, power+se, lty = 3)
lines(a, power-se, lty = 3)
```

For $1 \leq \alpha \leq 20$ ($\alpha$ denotes the parameter of Beta distribution) the power curve always below the horizontal line corresponding to $\alpha$=0.10, which means to reject the null hypotheses.

Then about the two heavy-tailed symmetric alternatives.

```{r results='asis'}
power_2 <- power_3 <- numeric(N)
for (j in 1:N) { #for each parameter of t distribution
e <- a[j]
stests_2 <- numeric(m)
for (i in 1:m) { #for each replicate
  x <- rt(n,e)
  stests_2[i] <- as.integer(abs(sk(x)) >= cv)
}
power_2[j] <- mean(stests_2)
}
for (j in 1:N) { #for each parameter of uniform distribution
e <- a[j]
stests_3 <- numeric(m)
for (i in 1:m) { #for each replicate
  x <- runif(n,0,e)
  stests_3[i] <- as.integer(abs(sk(x)) >= cv)
}
power_3[j] <- mean(stests_3)
}
plot(a,power, type = "l", xlab = "parameter", ylim = c(0,1),ylab="power")
lines(a,power_2,lty=2)
lines(a,power_3,lty=3)
abline(h = alpha, lty = 4)
legend("topright",1,c("normal","t","unif"),lty=c(1,2,3),inset=.02)
```

We can observe the power curve of uniform distribution with differrent thresholds is far close to the horizontal line corresponding to $\alpha$=0, which absolutely means to reject the null hypotheses. On the contrary, the t distribution shows acceptance of the null hypotheses and its power curve decreases with degrees of freedom increases. 

In conclusion, $t(\nu)$ is suitable for the skewness test of normality.

* ###Wonder

Consider the underlying reason for the tendency of t distribution power curve.


##Question **6.A**

Use Monte Carlo simulation to investigate whether the empirical Type I error rate of the t-test is approximately equal to the nominal significance level $\alpha$, when the sampled population is non-normal. The t-test is robust to mild departures from normality. Discuss the simulation results for the cases where the sampled population is (i) $\chi^{2}(1)$, (ii) Uniform(0,2), and (iii) Exponential(rate=1). In each case, test $H_{0}: \mu=\mu_{0}$ vs $H_{0}: \mu \neq \mu_{0}$ , where $\mu_{0}$ is the mean of $\chi^{2}(1)$, Uniform(0,2), and Exponential(1), respectively.

##Answer for **6.A**

* ###Basic idea

Suppose that $X_{1}, \ldots, X_{20}$ is a random sample from a $N\left(\mu, \sigma^{2}\right)$ distribution.
Test $H_{0}$ : ?? = 1 $H_{1}$ : ?? > 1 at $\alpha$ = 0.05. Under the null hypothesis,
$$
T^{*}=\frac{\bar{X}-1}{S / \sqrt{20}} \sim t(19)
$$
where t(19) denotes the Student t distribution with 19 degrees of freedom. Large values of $T^{*}$ support the alternative hypothesis.

Generate samples from non-normal distribution ,say (i) $\chi^{2}(1)$, (ii) Uniform(0,2), (iii) Exponential(rate=1), and see whether the empirical Type I error rate of the t-test is approximately equal to the nominal significance level $\alpha$.

* ###Code

```{r results='asis'}
rm(list=ls())
# remove variables
set.seed(0321)
n <- 20
# sample size
m<-1e4
# number of replication
p <- matrix(0,nrow=m,ncol=3)
p.hat <- numeric(3)
alpha<-.05
mu0 <- 1

for(j in 1:m){
  x <- matrix(0,n,3)
  x[,1] <- rchisq(n,1) # random numbers from chi-square distribution
  x[,2] <- runif(n,0,2) # random numbers from uniform distribution
  x[,3] <- rexp(n,1) # random numbers from exponential distribution
  for (k in 1:3){
    ttest <- t.test(x[,k],alternative="greater",mu=mu0)
    p[j,k] <- ttest$p.value
  }
}
p.hat <- colMeans(p<alpha)
c<-matrix(p.hat,1,3)
#Monte Carlo estimates of confidence level
colnames(c)<-c("Chi-square","Uniform","Exponential")
rownames(c)<-"p.hat"
pander::pandoc.table(c)
```

We can observe the empirical Type I error rate of samples generate from $\chi^{2}(1)$ and Exponential(rate=1) are far from the nominal significance level $\alpha$, which is said to be conservative, while the Uniform(0,2) sample have a empirical Type I error rate approximately equal to the nominal significance level $\alpha$.



##Discussion 

If we obtain the power for two methods under a particular simulation setting with 10,000 experiments: say,0.651 for one method and 0.676 for another method. Can we say the powers are different at 0.05 level?

##Answer

* ###Basic idea

power$_{-}1$ denotes the empirical power for method 1 and power$_{-}2$ denotes the empirical power for the other.

letting $n_{1}=$ power $_{-} 1 * 10,000$,$n_{2}=$ power $_{-} 2 * 10,000$

First, we know that $n_{1} \sim b\left(10000, p_{1}\right)$ and $n_{2} \sim b\left(10000, p_{2}\right)$, where $p_{1}$ denotes the power for method 1 and $p_{2}$ denotes the power for method 2.

Thus, test null hypothesis **$H_{0}$ : $p_{1}$=$p_{2}$**.

Further, the two empirical power both are generated from a particular simulation setting so paired-t test is an appropriate choice to test our hypothesis.

Also, to test our hypothesis, we need more empirical powers for the two methods.

##Question **7.8**

Refer to Exercise 7.6. Efron and Tibshirani discuss the following example. The five-dimensional scores data have a $5 ?? 5$ covariance matrix $\Sigma$, with positive eigenvalues $\lambda_{1}>\cdots>\lambda_{5}$. In principal components analysis,
$$
\theta=\frac{\lambda_{1}}{\sum_{j=1}^{5} \lambda_{j}}
$$
measures the proportion of variance explained by the first principal component. Let $\hat{\lambda}_{1}>\cdots>\hat{\lambda}_{5}$ be the eigenvalues of $\hat{\Sigma}$, where $\hat{\Sigma}$ is the MLE of $\Sigma$. Compute the sample estimate
$$
\hat{\theta}=\frac{\hat{\lambda}_{1}}{\sum_{j=1}^{5} \hat{\lambda}_{j}}
$$
of $\theta$. Use jackknife to estimate the bias and standard error of $\hat{\theta}$.

##Answer for **7.8**

* ###Basic idea

The MLE of $\Sigma$ shall be deemed as the adjusted covariance matrix, saying $\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{n}$ instead of $\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{n-1}$.

* ###Flowchart

1. Obtain sample size $n$ for jackknife.

2. Compute the value of $\hat{\theta}$ using given data.

3. Obtain $\hat{\theta}_{(i)}$ ($i=1, \ldots, n$).

4. Compute the bias $E(\hat{\theta})-\theta_{0}$.

5. Compute the standard error $\widehat{\operatorname{var}}(\hat{\theta})$.

* ###Code

```{r results='asis'}
rm(list=ls())
# remove variables
library(bootstrap)
# to obtain data
n<-nrow(scor)
# sample size for jackknife
theta.j<-numeric(n)
lamda.h<-eigen((n-1)*cov(scor)/n,only.values=T)$values
theta.h<-max(lamda.h)/sum(lamda.h)
# value of theta.hat
for(i in 1:n){
  y<-scor[-i,]
  x<-(n-2)*cov(y)/(n-1)
  # MLE of sigma
  lamda<-eigen(x,only.values=T)$values
  theta.j[i]<-max(lamda)/sum(lamda)
}
# theta.hat with jackknife
bias.j<-(n-1)*(mean(theta.j)-theta.h)
# bias of theta.hat with jackknife
se.j <- sqrt((n-1)*mean((theta.j-mean(theta.j))^2))
# standard error of theta.hat with jackknife
c<-matrix(round(c(original=theta.h,bias.jack=bias.j,se.jack=se.j),3),1,3)
colnames(c)<-c("original","basic.jack","se.jack")
pander::pandoc.table(c)
```

* ###Wonder

What can be done to reduce the bias and standard error of $\hat{\theta}$?


##Question **7.10**

In Example 7.18, leave-one-out (n-fold) cross validation was used to select the best fitting model. Repeat the analysis replacing the Log-Log model with a cubic polynomial model. Which of the four models is selected by the
cross validation procedure? Which model is selected according to maximum adjusted $R^{2}$ ?

##Answer for **7.10**

* ###Basic idea

We want to find a suitale relationship between chemical and magnetic. The candidate models as below:

Linear:$Y=\beta_{0}+\beta_{1} X+\epsilon$

Quadratic:$Y=\beta_{0}+\beta_{1} X+\beta_{2} X^{2}+\epsilon$

Exponential:$\log (Y)=\beta_{0}+\beta_{1} X+\epsilon$

cubic polynomial:$Y=\beta_{0}+\beta_{1} X+\beta_{2} X^{2}+\beta_{3} X^{3}+\epsilon$

Estimate prediction error by n-fold (leave-one-out) cross validation and the adjusted $R^{2}$ for each model, select the best one according to two principle respectively.


* ###Flowchart

1. For $k=1, \ldots, n$, let observation $\left(x_{k}, y_{k}\right)$ be the test point and use the remaining observations to fit the model.

2. Fit the model(s) using only the $n-1$ observations in the training set, $\left(x_{i}, y_{i}\right)$, $i \neq k$.

3. Compute the predicted response $\hat{y}_{k}$ for the test point.

4. Compute the prediction error $e_{k}=y_{k}-\hat{y}_{k}$.

5. Summary the data for each model and obtain the adjusted $R^{2}$.


* ###Code


```{r results='asis'}
rm(list=ls())
# remove variables
library(DAAG);attach(ironslag)
n <- length(magnetic) #in DAAG ironslag
er1 <- er2 <- er3 <- er4 <- numeric(n)
# for n-fold cross validation
# fit models on leave-one-out samples
for (k in 1:n) {
  y <- magnetic[-k]
  x <- chemical[-k]
  J1 <- lm(y ~ x)
  yhat1 <- J1$coef[1] + J1$coef[2] * chemical[k]
  er1[k] <- magnetic[k] - yhat1
  J2 <- lm(y ~ x + I(x^2))
  yhat2 <- J2$coef[1] + J2$coef[2] * chemical[k] +J2$coef[3] * chemical[k]^2
  er2[k] <- magnetic[k] - yhat2
  J3 <- lm(log(y) ~ x)
  logyhat3 <- J3$coef[1] + J3$coef[2] * chemical[k]
  yhat3 <- exp(logyhat3)
  er3[k] <- magnetic[k] - yhat3
  J4 <- lm(y ~ x+I(x^2)+I(x^3))
  yhat4 <- J4$coef[1] + J4$coef[2] * chemical[k]+J4$coef[3] * chemical[k]^2+J4$coef[4] * chemical[k]^3
  er4[k] <- magnetic[k] - yhat4
}
b<-matrix(c(mean(er1^2), mean(er2^2), mean(er3^2), mean(er4^2)),1,4)
colnames(b)<-c("Linear","Quadratic","Exponential","Cubic polynomial model")
rownames(b)<-"Error"
pander::pandoc.table(b)
```

As shown above, Model 2, the quadratic model, would be the best fit for the data, according to the prediction error criterion.

Then let us plot the predicted response with the data for each model.

```{r}
#par(mfrow=c(2,2))
data(ironslag,package="DAAG")
a <- seq(10, 40, .1) #sequence for plotting fits
L1 <- lm(magnetic ~ chemical)
plot(chemical, magnetic, main="Linear", pch=16)
yhat1 <- L1$coef[1] + L1$coef[2] * a
lines(a, yhat1, lwd=2)
L2 <- lm(magnetic ~ chemical + I(chemical^2))
plot(chemical, magnetic, main="Quadratic", pch=16)
yhat2 <- L2$coef[1] + L2$coef[2] * a + L2$coef[3] * a^2
lines(a, yhat2, lwd=2)
L3 <- lm(log(magnetic) ~ chemical)
plot(chemical, magnetic, main="Exponential", pch=16)
logyhat3 <- L3$coef[1] + L3$coef[2] * a
yhat3 <- exp(logyhat3)
lines(a, yhat3, lwd=2)
L4 <- lm(magnetic ~ chemical + I(chemical^2)+I(chemical^3))
plot(chemical, magnetic, main="Cubic polynomial model", pch=16)
yhat4 <- L4$coef[1] + L4$coef[2] * a + L4$coef[3] * a^2+L4$coef[4] * a^3
lines(a, yhat4, lwd=2)
```

Now obtain the adjusted $R^{2}$ for each model.

```{r results='asis'}
AR<-numeric(4)
AR[1]<-summary(L1)[9]
AR[2]<-summary(L2)[9]
AR[3]<-summary(L3)[9]
AR[4]<-summary(L4)[9]
ajust_R2<-round(as.numeric(AR),3)
c<-matrix(ajust_R2,1,4)
colnames(c)<-c("Linear","Quadratic","Exponential","Cubic polynomial model")
rownames(c)<-"adj.r.squared"
pander::pandoc.table(c)

```

According to the maximum adjusted $R^{2}$ criterion, the quadratic model, would also be the best fit for the data.

##Question **8.3**

The Count 5 test for equal variances in Section 6.4 is based on the maximum number of extreme points. Example 6.15 shows that the Count 5 criterion is not applicable for unequal sample sizes. Implement a permutation test for equal variance based on the maximum number of extreme points that applies when sample sizes are not necessarily equal.

##Answer for **8.3**

* ###Basic idea

Here is a test of equal variance.
$$
H_{0}: \sigma_{1}^{2}=\sigma_{2}^{2} \text { vs } H_{a}: \sigma_{1}^{2} \neq \sigma_{2}^{2}
$$
For testing the equality of variance of two sample, an observation is considered $extreme$ if it is not within the range of the other sample. We will implement a permutation test for equal variance based on the maximum number of extreme points that applies when sample sizes are not necessarily equal. At the same time, since Ball could be more powerful for non-location family distribution, it will be performed for comparison.

* ###Flowchart

1. Set two different sample sizes.

2. Set two different variances.

3. Write a function to obtain maximum number of extreme points which can be used in function $boot$.

4. Write a function to obtain p-value of permutation test.

5. Use Monte Carlo method to obtain p-values of the two test under two sorts of condition, saying $H_{0}$ is ture and $H_{0}$ is false, respectively.

* ###Code

```{r results='asis'}
rm(list=ls())
# remove variables
library(boot);library(Ball)
# to obtain function
set.seed(234)
m<-1e3;R<-999
n1<-20;n2<-30
# different sample sizes
N<-c(n1,n2)
mu1<-mu2<-0
sigma1<-sigma2<-1
# equal variance
sigma3<-2
# unequal variance
Tn <- function(d,ix,sizes){
  n1<-sizes[1];n2<-sizes[2]
  n<-n1+n2
  ii<-ix[1:n1];jj<-ix[(n1+1):n]
  outx<-sum(d[ii]>max(d[jj]))+sum(d[ii]<min(d[jj]))
  outy<-sum(d[jj]>max(d[ii]))+sum(d[jj]<min(d[ii]))
  return(max(outx,outy))
}
# function to obtain maximum number of extreme points
myeq.test<-function(d){
  boot.obj<-boot(data=d,R=R,statistic=Tn,sim="permutation",sizes=N)
  ts<-c(boot.obj$t0,boot.obj$t)
  p.value<-mean(ts >= ts[1])
  return(p.value)
}
# function to obtain p.value of permutation test
p.values<-matrix(NA,m,4)
for(i in 1:m){
  x<-rnorm(n1,mu1,sigma1)
  y<-rnorm(n2,mu2,sigma2)
  z<-rnorm(n2,mu2,sigma3)
  d1<-c(x,y);d2<-c(x,z)
  p.values[i,1]<-bd.test(x=x,y=y,R=999,seed=i*123)$p.value
  # p.value of Ball test with H0 being TURE 
  p.values[i,2]<-myeq.test(d1)
  # p.value of permutation test with H0 being TURE 
  p.values[i,3]<-bd.test(x=x,y=z,R=999,seed=i*123)$p.value
  # p.value of Ball test with H0 being FALSE
  p.values[i,4]<-myeq.test(d2)
  # p.value of permutation test with H0 being FALSE
}
c<-matrix(colMeans(p.values),1,4)
colnames(c)<-c("bd.test(H0=T)","myeq.test(H0=T)","bd.test(H0=F)","myeq.test(H0=F)")
rownames(c)<-"p.value"
pander::pandoc.table(c)
```

As is shown above, under the circumstances of equal variance, both Ball-test and permutation-test can not reject the null hypothesis, while permutaion-test being more likely to accept $H_{0}$. Nevertheless, when the null hypothesis was false, neither Ball nor permutation test would perform very well but the latter would show a smaller p-value.

* ###Wonder

Perform one permutation test to observe p-value more intuitively.

```{r}
set.seed(111)
ox<-rnorm(n1,mu1,sigma1)
oy<-rnorm(n2,mu2,sigma2)
od<-c(ox,oy)
obj.o<-boot(data=od,R=999,statistic=Tn,sim="permutation",sizes=N)
tb <- c(obj.o$t,obj.o$t0)
hist(tb, freq=FALSE, main="",xlab="replicates of maximum number of extreme points")
abline(v=obj.o$t0,col='red',lwd=2)
```

We can observe the original number of extreme points is 4<5, which is consistent with the result under Count 5 criterion.

##Question **sides**

Power comparison (distance correlation test versus ball covariance test)

Model 1:$Y=X / 4+e$

Model 2:$Y=X / 4 \times e$

$x \sim N\left(0_{2}, l_{2}\right), e \sim N\left(0_{2}, l_{2}\right), X$ and $e$ are independent

##Answer for **sides**

* ###Flowchart

1. Set number of p-value M for power.

2. Set sample size sequence.

3. For $\dot{l}^{t h}$ p-values With $j^{t h}$ sample size n, obtain $x$ $e$ from $\ N\left(0_{2}, l_{2}\right)$ independently. 

4. Compute $y_{1}$ $y_{2}$ with two models.

5. Obtain $\dot{l}^{t h}$ p-values of two models using two sorts of test.

6. Obtain $j^{t h}$ power with $j^{t h}$ sample size.


* ###Code

I am so sorry for the awful answer. I have tried my best but my laptop can not run smoothly and I cannot figure what's wrong with my second answer.So I put my procedure in the comment line. If you were so kind to debug my procedure, I would appreciate it very well. Apologize again.

```{r results='asis'}
#rm(list=ls())
## remove variables
#library(Ball);library(MASS);library(boot)
#par(mfrow=c(1,2))
##set.seed(123)
#M<-100
##sample size for power
#alpha <- 0.1
#sigma<-matrix(c(1,0,0,1),2,2)
#dCov <- function(x, y){
#  x <- as.matrix(x)
#  y <- as.matrix(y)
#  n <- nrow(x)
#  m <- nrow(y)
#  if (n != m || n < 2) stop("Sample sizes must agree")
#  if (! (all(is.finite(c(x, y)))))
#  stop("Data contains missing or infinite values")
#  Akl <- function(x) {
#    d <- as.matrix(dist(x))
#    m <- rowMeans(d)
#    M <- mean(d)
#    a <- sweep(d, 1, m)
#    b <- sweep(a, 2, m)
#    return(b + M)
#  }
#  A<-Akl(x)
#  B<-Akl(y)
#  dCov<-sqrt(mean(A*B))
#  dCov
#}
#ndcov2 <- function(z, ix, dims) {
##dims contains dimensions of x and y
#p <- dims[1]
#q1 <- dims[2] + 1
#d <- p + dims[2]
#x <- z[ , 1:p] #leave x as is
#y <- z[ix, q1:d] #permute rows of y
#return(nrow(z) * (dCov(x,y))^2)
#}
#size<-seq(10,205,by=5)
## sample size sequence
#for(j in 1:40){
#  n<-size[j]
#  pow<-matrix(NA,40,4)
#  p.values<-matrix(NA,M,4)
#  for(i in 1:M){
#    set.seed(i*j*321)
#    x<-mvrnorm(n,rep(0,2),sigma)
#    e<-mvrnorm(n,rep(0,2),sigma)
#    y1<-x/4+e
#    y2<-x/4*e
#    p.values[i,1]<-bcov.test(x=x,y=y1,R=10,seed=i*121)$p.value
#    # p-value of Ball correlation test of model 1
#    m1<-matrix(cbind(x,y1),n,4)
#    boot.o1<-boot(data=m1,statistic=ndcov2,R=99,sim="permutation",dims=c(2,2))
#    t.b1<-c(boot.o1$t0,boot.o1$t)
#    p.values[i,2]<-mean(t.b1>=t.b1[1])
#    # p-value of distance correlation test of model 1
#    p.values[i,3]<-bcov.test(x=x,y=y2,R=99,seed=i*123)$p.value
#    # p-value of Ball correlation test of model 2
#    m2<-matrix(cbind(x,y2),n,4)
#    boot.o2<-boot(data=m2,statistic=ndcov2,R=99,sim="permutation",dims=c(2,2))
#    t.b2<-c(boot.o2$t0,boot.o2$t)
#    p.values[i,4]<-mean(t.b2>=t.b2[1])
#    # p-value of distance correlation test of model 2
#  }
#  pow[j,] <- colMeans(p.values<alpha)
#}
#plot(size,pow[,1], type = "l", xlab = "n", ylim = c(0,1),ylab="power",main="Model 1")
#  lines(size,pow[,2],lty=2)
#  legend("topleft",1,c("Ball","dCov"),lty=c(1,2),inset=.02)
#plot(size,pow[,3], type = "l", xlab = "n", ylim = c(0,1),ylab="power",main="Model 2")
#lines(size,pow[,4],lty=2)
#legend("topleft",1,c("Ball","dCov"),lty=c(1,2),inset=.02)
```

##Question **9.4**

Implement a random walk Metropolis sampler for generating the standard Laplace distribution (see Exercise 3.2). For the increment, simulate from a normal distribution. Compare the chains generated when different variances
are used for the proposal distribution. Also, compute the acceptance rates of each chain.

##Answer for **9.4**

* ###Basic idea

Markov Chain Monte Carlo methods estimate the integral by Monte Carlo integration, and the Markov Chain provides the sampler that generates the random observations from the target distribution, saying the standard Laplace distribution here.

The standard Laplace distribution has density
$$
f(x)=\frac{1}{2} e^{-|x|}, x \in \mathbb{R}
$$

We choose a symmetric proposal distribution as $N\left(X_{t}, \sigma^{2}\right)$, in that the generated chain from it is able to converge to the standard Laplace distribution and enjoy irreducibility, positive recurrence, and aperiodicity.

* ###Flowchart

1. Generate $Y$ from $N\left(X_{t}, \sigma^{2}\right)$.

2. Given initial value $X_{0}$.

3. Generate $U$ from Uniform(0,1).

4. If
$$
U \leq \frac{f(Y)}{f\left(X_{t}\right)}
$$

accept $Y$ and set $X_{t+1}=Y$ ; otherwise set $X_{t+1}=X_{t}$.

5. Increment $t$ until the chain has converged to a stationary distribution.

6. Repeating the simulation with different choices of $\sigma$

* ###Code

```{r results='asis'}
rm(list=ls())
# remove variables
library(GeneralizedHyperbolic)
# to obtain function
set.seed(123)
N <- 2000
# iteration time
sigma <- c(.05, .5, 2, 16)
# different choices of sigma
x0<-25
# initial setting
myrw.Metropolis <- function(a, x0, N) {
  x <- numeric(N)
  x[1] <- x0
  u <- runif(N)
  k <- 0
  for (i in 2:N) {
    y <- rnorm(1, x[i-1], a)
    if (u[i] <= (dskewlap(y) / dskewlap(x[i-1])))
      x[i] <- y else {
        x[i] <- x[i-1]
        k <- k + 1
      }
  }
  return(list(x=x, k=k))
}
# function to generate the chain
r1 <- myrw.Metropolis(sigma[1], x0, N)
r2 <- myrw.Metropolis(sigma[2], x0, N)
r3 <- myrw.Metropolis(sigma[3], x0, N)
r4 <- myrw.Metropolis(sigma[4], x0, N)
c<-matrix(c(r1$k/N, r2$k/N, r3$k/N, r4$k/N),1,4)
colnames(c)<-c("sigma=0.05","sigma=0.5","sigma=2","sigma=16")
rownames(c)<-"rejection rate"
pander::pandoc.table(c)
```

As we can observe in the table, both the second and third chain have a rejection rate in the range [0.15, 0.5].

Then we will show what can happen to the random walk Metropolis sampler when the variance of proposal distribution change.

```{r}
#par(mfrow=c(2,2))
plot(r1$x,type='l',ylab="X",xlab=expression(paste(sigma,'=0.05')))
# generated sample vs the time index when the variance of proposal distribution is 0.05
plot(r2$x,type='l',ylab="X",xlab=expression(paste(sigma,'=0.5')))
# generated sample vs the time index when the variance of proposal distribution is 0.5
 abline(h=-3,col='red',lwd=.3)
 abline(h=3,col='red',lwd=.3)
 # convergence range
plot(r3$x,type='l',ylab="X",xlab=expression(paste(sigma,'=2')))
# generated sample vs the time index when the variance of proposal distribution is 4
abline(h=-3,col='red',lwd=.3)
abline(h=3,col='red',lwd=.3)
# convergence range
plot(r4$x,type='l',ylab="X",xlab=expression(paste(sigma,'=16')))
# generated sample vs the time index when the variance of proposal distribution is 16
abline(h=-3,col='red',lwd=.3)
abline(h=3,col='red',lwd=.3)# convergence range 

```

As is shown above, under the circumstances of proposal distribution with $N\left(X_{t}, 0.05^{2}\right)$, Chain 1 has not converged to the target in 2000 iterations. The chain in second($\sigma=0.5$) and third($\sigma=2$) plot converging to the target distribution while the former requires a little longer burn-in period. Finally, in the fourth plot, where $\sigma$= 16, most of the candidate points are rejected. The fourth chain converges, but it is inefficient.

* ###Wonder

We want to observe the relationship between generated sample and the target distribution in a more intuitive way. So the QQ plot and histogram of the generated sample with $N\left(X_{t}, 0.5^{2}\right)$ and $N\left(X_{t}, 2^{2}\right)$ density superimposed are shown below respectively.

```{r}
#par(mfrow=c(2,2))
b <- 201 ; d<-101
#discard the burnin sample
y1 <- r2$x[b:N]
y2 <- r3$x[d:N]
a <- ppoints(100)
QR <- qskewlap(1-a) 
# quantiles of Rayleigh
Q1 <- quantile(y1, a)
Q2 <- quantile(y2, a)
qqplot(QR, Q1, main="",xlab="standard Laplace distibution", ylab="Sample Quantiles")
hist(y1, breaks="scott", main="", xlab="", freq=FALSE)
lines(QR, dskewlap(QR))
qqplot(QR, Q2, main="",xlab="standard Laplace distibution", ylab="Sample Quantiles")
hist(y2, breaks="scott", main="", xlab="", freq=FALSE)
lines(QR, dskewlap(QR))
```

##Question **11.1**

The natural logarithm and exponential functions are inverses of each other, so that mathematically $\log (\exp x)=\exp (\log x)=x$. Show by example that this property does not hold exactly in computer arithmetic. Does the identity hold with near equality? (See all.equal.)

##Answer for **11.1**

* ###Basic idea

It is well known that there are fundamental differences between mathematical calculations and computer calculations. Mathematical ideas such as limit, supremum, infimum, etc. cannot be exactly reproduced in the computer. Therefore, inverse transformation can not obtain the original result.

* ###Flowchart

1. Set the value of $x$.

2. Compute $\log \left(e^{x}\right)$.

3. Compute $e^{\log (x)}$.

4. Compare the two results with $x$.


* ###Code

```{r results='asis'}
rm(list=ls())
# remove variables
x<-1e-50
y1<-log(exp(x))
y2<-exp(log(x))
c<-matrix(c(x,y1,y2),1,3)
colnames(c)<-c("x","log(exp(x))","exp(log(x))")
rownames(c)<-"value"
pander::pandoc.table(c)
```

We can observe $\log \left(\exp \left(10^{-50}\right)\right) \neq \exp \left(\log \left(10^{-50}\right)\right)=10^{-50}$

Now let us see how $all.equal$ will perform.

```{r}
isTRUE(all.equal(x,y1))
```
Thus we know $\log \left(\exp \left(10^{-50}\right)\right)=\exp \left(\log \left(10^{-50}\right)\right)=10^{-50}$ hold with near equality.

* ###Wonder

How to avoid the bug of computer calculations?

In view of the comparision of $\log \left(\exp \left(10^{-50}\right)\right)$ and $\exp \left(\log \left(10^{-50}\right)\right)$, appropriate use of function$log$ might be an effective method to obtain correct computation.  


##Question **11.5**

Write a function to solve the equation
$$
\begin{aligned} \frac{2 \Gamma\left(\frac{k}{2}\right)}{\sqrt{\pi(k-1)} \Gamma\left(\frac{k-1}{2}\right)} \int_{0}^{c_{k-1}}\left(1+\frac{u^{2}}{k-1}\right)^{-k / 2} d u \\= \frac{2 \Gamma\left(\frac{k+1}{2}\right)}{\sqrt{\pi k} \Gamma\left(\frac{k}{2}\right)} \int_{0}^{c_{k}}\left(1+\frac{u^{2}}{k}\right)^{-(k+1) / 2} d u \end{aligned}
$$
for a, where
$$
c_{k}=\sqrt{\frac{a^{2} k}{k+1-a^{2}}}
$$
Compare the solutions with the points $A(k)$ in Exercise 11.4.

##Answer for **11.5**

* ####Basic idea

As we know, $X \sim t(n)$ when $X$ have the density function
$$
f(x)=\frac{\Gamma\left(\frac{n+1}{2}\right)}{\sqrt{n \pi} \Gamma\left(\frac{n}{2}\right)}\left(1+\frac{x^{2}}{n}\right)^{-\frac{n+1}{2}}
$$
Inspired by 11.4, we want to see the performance of two curves in section [0,sqrt(k)] for k=4 and 9.

Then find the possible section that exists root.


* ###Code

```{r}
rm(list=ls())
K<-c(4,9)
#par(mfrow=c(1,2))
A1<-seq(0,sqrt(K[1]),0.01)
A2<-seq(0,sqrt(K[2]),0.01)
g1<-function(x){(1+(x^2)/(k-1))^(-k/2)}
g2<-function(x){(1+(x^2)/k)^(-(k+1)/2)}
ck<-function(k,a){
  return(sqrt(a^2*k/(k+1-a^2)))
}
y11<-y12<-numeric(length(A1))
y21<-y22<-numeric(length(A2))
for(i in 1:length(A1)){
  a<-A1[i]
  k<-K[1]
y11[i]<-2*gamma(k/2)/(sqrt(pi*(k-1))*gamma((k-1)/2))*integrate(g1, lower=0, upper=ck(k-1,a))$value
y12[i]<-2*gamma((k+1)/2)/(sqrt(pi*k)*gamma(k/2))*integrate(g2,lower=0,upper=ck(k,a))$value
}
plot(A1,y11,type='l',col='red')
lines(A1,y12,col='blue')
for(i in 1:length(A2)){
  a<-A2[i]
  k<-K[2]
y21[i]<-2*gamma(k/2)/(sqrt(pi*(k-1))*gamma((k-1)/2))*integrate(g1, lower=0, upper=ck(k-1,a))$value
y22[i]<-2*gamma((k+1)/2)/(sqrt(pi*k)*gamma(k/2))*integrate(g2,lower=0,upper=ck(k,a))$value
}
plot(A2,y21,type='l',col='red')
lines(A2,y22,col='blue')

```

We choose sqrt(k)/2+1 as the upper of our area to extract root.

```{r results='asis'}
rm(list=ls())
# remove variables
K<-c(4:25,100)
# different k
N<-length(K)
f1 <- function(x) {
  2*gamma(k/2)/(sqrt(pi*(k-1))*gamma((k-1)/2))*(1+(x^2)/(k-1))^(-k/2)
}
# integrand 1
f2<-function(x) {
  2*gamma((k+1)/2)/(sqrt(pi*k)*gamma(k/2))*(1+(x^2)/k)^(-(k+1)/2)
}
# integrand 2
ck<-function(k,a){
  return(sqrt(a^2*k/(k+1-a^2)))
}
# integral upper limit
A<-numeric(N)
for(i in 1:N){
  k<-K[i]
  ss<-uniroot(function(a){
  integrate(f1, lower=0, upper=ck(k-1,a))$value-integrate(f2,lower=0,upper=ck(k,a))$value
    },lower=0.5,upper=sqrt(k)/2+1)
  A[i]<-ss$root
}
```

So, roots of the equation for k=4:25;100 are stored in vector A.

Compare our solution with 11.4, combining with property of distribution function, we can see A(k) in 11.4 is almost equal to our resluts. 


##Question **3**

A-B-O blood type problem

Let the three alleles be A, B, and O with allele frequencies p,q, and r. The 6 genotype frequencies under HWE and complete counts are as follows.
$$
\begin{array}{|c|c|c|c|c|c|}\hline \text { Genotype } & {A A} & {B B} & {O O} & {A O} & {B O} & {A B} & {S u m} \\ \hline \text { Frequency } & {p^{\sim} 2} & {q^{\sim 2}} & {r^{\sim 2}} & {2 p r} & {2 q r} & {2 p q} & {1} \\ \hline \text { Count } & {n A A} & {n B B} & {n O O} & {n A O} & {n B O} & {n A B} & {n} \\ \hline\end{array}
$$
Observed data: $n_{A}=n_{A A}+n_{A O}=28$(A-type),$n_{B .}=n_{B B}+n_{B O}=24$(B-type), $n_{O O}=41$ (O-type),$n_{A B}=70$ (AB-type).

1. Use EM algorithm to solve MLE of p and q (consider missing data $n_{A A}$ and $n_{B B}$ ).

2. Show that the log-maximum likelihood values in M-steps are increasing via line plot.

##Answer for **3**

* ###Basic idea

Observed data likelyhood:

$$
\begin{array}{l}{L\left(p, q | n_{A}, n_{B}, n_{O O}, n_{A B}\right)=\left(p^{2}+2 p r\right)^{n_{A}}\left(q^{2}+2 q r\right)^{n_{B} .}\left(r^{2}\right)^{n_{O O}}(2 p q)^{n_{4 B}}} \end{array}
$$

Complete data likelyhood:

$$
\begin{array}{l}{L\left(p, q | n_{A A}, n_{B B}, n_{O O}, n_{A O}, n_{B O}\right)=\left(p^{2}\right)^{n_{4 A}}\left(q^{2}\right)^{n_{B 8}}\left(r^{2}\right)^{n_{O O}}(2 p r)^{n_{B O}}(2 q r)^{n_{S O}}(2 p q)^{n_{A B}}}\end{array}
$$

After logarithm:

$$
\begin{aligned} \ln L=& 2 n_{A A} \log (p)+2 n_{B B} \log (q)+2 n_{O O} \log (r)+\left(n_{\cdot A}-n_{A A}\right) \log (2 p r) \\ &+\left(n_{B .}-n_{B B}\right) \log (2 q r)+n_{A B} \log (2 p q) \ \end{aligned}
$$

$E$xpectation step:

$$
\begin{aligned} \ E_{\hat{p}_{0}, \hat{q}_{0}}(\ln L)=& \frac{2 \hat{p}_{0}}{\hat{p}_{0}^{2}+2 \hat{p}_{0} \hat{r}_{0}} n_{A} \log (p)+\frac{2 \hat{q}_{0}}{\hat{q}_{0}^{2}+2 \hat{q}_{0} \hat{r}_{0}} n_{B} \log (q)+2 n_{O O} \log (r) \\ &+n_{A} \cdot \frac{2 \hat{p}_{0} \hat{r}_{0}}{\hat{p}_{0}^{2}+2 \hat{p}_{0} \hat{r}_{0}} \log (2 p r)+n_{B} \cdot \frac{2 \hat{q}_{0} \hat{r}_{0}}{\hat{q}_{0}^{2}+2 \hat{q}_{0} \hat{r}_{0}} \log (2 q r)+n_{A B} \log (2 p q) \end{aligned}
$$

$\hat{r}_{0}=1-\hat{p}_{0}-\hat{q}_{0}$ $r=1-p-q$

Thus we can obtain $\hat{p}_{1}$ and $\hat{q}_{1}$ though MLE

Update the estimates and iteratively repeat the $E$ step and $M$ steps until the algorithm converges to some criterion.

* ###Code

```{r results='asis'}
rm(list=ls())
# remove variables
library(rootSolve)
N <- 2e2
# max. number of the iteration
nA<-28;nB<-24;nOO<-41;nAB<-70
G <- c(.5,.4)
# initial estimates 
tol <- .Machine$double.eps^0.5
G.old <- G+1
E <- numeric(N)
for(j in 1:N){
  E[j]<-2*G[1]*nA*log(G[1])/(2-G[1]-2*G[2])+2*G[2]*nB*log(G[2])/(2-G[2]-2*G[1])+2*nOO*log(1-G[1]-G[2])+nA*(2-2*G[1]-2*G[2])*log(2*G[1]*(1-G[1]-G[2]))/(2-G[1]-2*G[2])+nB*(2-2*G[1]-2*G[2])*log(2*G[2]*(1-G[1]-G[2]))/(2-G[2]-2*G[1])+nAB*log(2*G[1]*G[2])
  model<-function(x){
    F1<-2*G[1]*nA/((2-G[1]-2*G[2])*x[1])-2*nOO/(1-x[1]-x[2])+nA*(2-2*G[1]-2*G[2])*(1-2*x[1]-x[2])/((2-G[1]-2*G[2])*x[1]*(1-x[1]-x[2]))-nB*(2-2*G[1]-2*G[2])/((2-G[2]-2*G[1])*(1-x[1]-x[2]))+nAB/x[1]
    F2<-2*G[2]*nB/((2-G[2]-2*G[1])*x[2])-2*nOO/(1-x[1]-x[2])-nA*(2-2*G[1]-2*G[2])/((2-G[1]-2*G[2])*(1-x[1]-x[2]))+nB*(2-2*G[1]-2*G[2])*(1-2*x[2]-x[1])/((2-G[2]-2*G[1])*x[2]*(1-x[1]-x[2]))+nAB/x[2]
    c(F1=F1,F2=F2)
  }
  ss<-multiroot(f=model,star=c(.1,.1))
  G<-ss$root
  # update p and q
  if (sum(abs(G-G.old)/G.old)<tol) break
  G.old<-G
}
print(G.old)
```

Therefore we obtained MLE of p and q, say 0.327 and 0.31 respectively.

Now let us show the log-maximum likelihood values in M-steps via line plot.

```{r}
plot(E[1:11],type='l',xlab="iteration time",ylab="likelihood value")
```

As shown above, we can observe that the log-maximum likelihood values are increasing in M-steps. 

##Question **11.1.2 Exercise 3**

Use both for loops and lapply() to fit linear models to the mtcars using the formulas stored in this list:

formulas <- list(

mpg ~ disp,

mpg ~ I(1 / disp),

mpg ~ disp + wt,

mpg ~ I(1 / disp) + wt

)

##Answer for **11.1.2 Exercise 3**

* ###Processes and Code

First, we use $for$ loops to fit linear models as we learned before and store the results in $fit1$.

```{r results='asis'}
rm(list=ls())
# remove variables
attach(mtcars)
formulas <- list(
mpg ~ disp,
mpg ~ I(1 / disp),
mpg ~ disp + wt,
mpg ~ I(1 / disp) + wt
)
fit11<-vector("list",length(formulas))
cof11<-matrix(0,4,3)
for(i in seq_along(formulas)){
  fit11[[i]]<-lm(formulas[[i]])
  cof11[i,1:length(fit11[[i]]$coefficients)]<-fit11[[i]]$coefficients
}
colnames(cof11)<-c("intercept","coefficient1","coefficient2")
rownames(cof11)<-c("mpg ~ disp","mpg ~ I(1 / disp)","mpg ~ disp + wt","mpg ~ I(1 / disp) + wt")
pander::pandoc.table(cof11)
```

Now we use $lapply$ to realize it.

```{r results='asis'}
fit12<-lapply(formulas,lm)
cof12<-matrix(0,4,3)
for(i in seq_along(formulas)){
  cof12[i,1:length(fit12[[i]]$coefficients)]<-fit12[[i]]$coefficients
}
colnames(cof12)<-c("intercept","coefficient1","coefficient2")
rownames(cof12)<-c("mpg ~ disp","mpg ~ I(1 / disp)","mpg ~ disp + wt","mpg ~ I(1 / disp) + wt")
pander::pandoc.table(cof12)
```


##Question **11.1.2 Exercise 4**

Fit the model mpg ~ disp to each of the bootstrap replicates of mtcars in the list below by using a for loop and lapply(). Can you do it without an anonymous function?

bootstraps <- lapply(1:10, function(i) {

rows <- sample(1:nrow(mtcars), rep = TRUE)

mtcars[rows, ]

})

##Answer for **11.1.2 Exercise 4**

* ###Processes and Code

First, we use $for$ loops to fit the model mpg ~ disp to each of the bootstrap replicates and show the coefficients.

```{r results='asis'}
bootstraps <- lapply(1:10, function(i) {
rows <- sample(1:nrow(mtcars), rep = TRUE)
mtcars[rows, ]
})
fit21<-vector("list",length(bootstraps))
cof21<-matrix(0,10,2)
for(i in seq_along(bootstraps)){
  fit21[[i]]<-lm(bootstraps[[i]]$mpg~bootstraps[[i]]$disp)
  cof21[i,]<-fit21[[i]]$coefficients
}
colnames(cof21)<-c("intercept","coefficient")
rownames(cof21)<-c("rep1","rep2","rep3","rep4","rep5","rep6","rep7","rep8","rep9","rep10")
pander::pandoc.table(cof21)
```

Then we use $lapply$ to fit the models.

```{r results='asis'}
fit22<-lapply(bootstraps,function(x){
  lm(x$mpg~x$disp)
})
cof22<-matrix(0,10,2)
for(i in seq_along(bootstraps)){
  cof22[i,]<-fit22[[i]]$coefficients
}
colnames(cof22)<-c("intercept","coefficient")
rownames(cof22)<-c("rep1","rep2","rep3","rep4","rep5","rep6","rep7","rep8","rep9","rep10")
pander::pandoc.table(cof22)
```

Finally, we try to do it without an anonymous function.

```{r results='asis'}
fit23<-lapply(bootstraps,lm,formula=mpg~disp)
cof23<-matrix(0,10,2)
for(i in seq_along(bootstraps)){
  cof23[i,]<-fit23[[i]]$coefficients
}
colnames(cof23)<-c("intercept","coefficient")
rownames(cof23)<-c("rep1","rep2","rep3","rep4","rep5","rep6","rep7","rep8","rep9","rep10")
pander::pandoc.table(cof23)
```

##Question **11.1.2 Exercise 5**

For each model in the previous two exercises, extract R2 using the function below.

rsq <- function(mod) summary(mod)$r.squared

##Answer for **11.1.2 Exercise 5**

* ###Processes and Code

For Exercise 3, we extract R2 for $lapply$ model.

```{r results='asis'}
rsq <- function(mod) summary(mod)$r.square
rsq1 <- lapply(fit12,rsq)
r_squared1<-matrix(rsq1,4,1)
colnames(r_squared1)<-"R_squared"
rownames(r_squared1)<-c("mpg ~ disp","mpg ~ I(1 / disp)","mpg ~ disp + wt","mpg ~ I(1 / disp) + wt")
pander::pandoc.table(r_squared1)
```

For Exercise 4, we extract R2 for $lapply$ model without an anonymous function.

```{r results='asis'}
rsq2 <- lapply(fit23,rsq)
r_squared2<-matrix(rsq2,10,1)
colnames(r_squared2)<-"R_squared"
rownames(r_squared2)<-c("rep1","rep2","rep3","rep4","rep5","rep6","rep7","rep8","rep9","rep10")
pander::pandoc.table(r_squared2)
```

##Question **11.2.5 Exercise 3**

The following code simulates the performance of a t-test for non-normal data. Use sapply() and an anonymous function to extract the p-value from every trial.

trials <- replicate(

100,

t.test(rpois(10, 10), rpois(7, 10)),

simplify = FALSE

)

Extra challenge: get rid of the anonymous function by using [[ directly.

##Answer for **11.2.5 Exercise 3**

We extract the p-value from every trial and take the first 5 for example.

```{r results='asis'}
rm(list=ls())
# remove variables
trials <- replicate(
100,
t.test(rpois(10, 10), rpois(7, 10)),
simplify = FALSE
)
p_value<-sapply(trials,function(x) return(x$p.value))
p<-matrix(p_value[1:5],5,1)
colnames(p)<-"p_value"
rownames(p)<-c("trial1","trial2","trial3","trial4","trial5")
pander::pandoc.table(p)
```

Also, we shall use $sapply()$ without an anonymous function.

```{r results='asis'}
p_value2<-sapply(trials,'[[',3)
p2<-matrix(p_value2[1:5],5,1)
colnames(p2)<-"p_value"
rownames(p2)<-c("trial1","trial2","trial3","trial4","trial5")
pander::pandoc.table(p2)
```

##Question **11.2.5 Exercise 7**

Implement mcsapply() , a multicore version of sapply() . Can you implement mcvapply() , a parallel version of vapply() ? Why or why not?

##Answer for **11.2.5 Exercise 7**

First, we implement a multicore version of sapply(), named as mcsapply().

```{r}
rm(list=ls())
# remove variables
library(parallel)
attach(mtcars)
cores <- detectCores()
cluster <- makePSOCKcluster(cores)
#  set up a local cluster
mcsapply <- function(x, f, ...) {
res <- parLapply(cluster, x, f)
simplify2array(res)
} 

```

To show the advantages of $mcsapply$, we take a realistic example.

```{r results='asis'}
boot_df <- function(x) x[sample(nrow(x), rep = T), ]
boot_lm <- function(i) {
  return(summary(lm(mpg ~ wt + disp, data = mtcars[sample(nrow(mtcars), rep = T), ]))$r.square)
}
T1<-system.time(sapply(1:500, boot_lm))
T2<-system.time(mcsapply(1:500, boot_lm))
c<-matrix(c(T1[1:3],T2[1:3]),2,3)
colnames(c)<-c("user","system","elapsed")
rownames(c)<-c("sapply","mcsapply")
pander::pandoc.table(c)
```

Then come the parallel version of vapply(). We do not think it is implementable in view of $vapply()$ shall check the length and type of results in appling function to each element in a list.

##Question **AR2**

Implement a random walk Metropolis sampler for generating the standard Laplace distribution (see Exercise 3.2). For the increment, simulate from a normal distribution. Compare the chains generated when different variances
are used for the proposal distribution. Also, compute the acceptance rates of each chain.

##Answer for **AR2**

* ###Basic idea

Markov Chain Monte Carlo methods estimate the integral by Monte Carlo integration, and the Markov Chain provides the sampler that generates the random observations from the target distribution, saying the standard Laplace distribution here.

The standard Laplace distribution has density
$$
f(x)=\frac{1}{2} e^{-|x|}, x \in \mathbb{R}
$$

We choose a symmetric proposal distribution as $N\left(X_{t}, \sigma^{2}\right)$, in that the generated chain from it is able to converge to the standard Laplace distribution and enjoy irreducibility, positive recurrence, and aperiodicity.

* ###Flowchart

1. Generate $Y$ from $N\left(X_{t}, \sigma^{2}\right)$.

2. Given initial value $X_{0}$.

3. Generate $U$ from Uniform(0,1).

4. If
$$
U \leq \frac{f(Y)}{f\left(X_{t}\right)}
$$

accept $Y$ and set $X_{t+1}=Y$ ; otherwise set $X_{t+1}=X_{t}$.

5. Increment $t$ until the chain has converged to a stationary distribution.

6. Repeating the simulation with different choices of $\sigma$

* ###Code

```{r results='asis'}
rm(list=ls())
# remove variables
library(GeneralizedHyperbolic)
# to obtain function
library(Rcpp)
# Attach R package "Rcpp"
#dir_cpp<-'Rcpp/'
# Can create source file in Rstudio
#sourceCpp(paste0(dir_cpp,"MetropolisC.cpp"))
library(microbenchmark)
set.seed(123)
N <- 2000
# iteration time
sigma <- c(.05, .5, 2, 16)
# different choices of sigma
x0<-25
# initial setting
myrw.Metropolis <- function(a, x0, N) {
  x <- numeric(N)
  x[1] <- x0
  u <- runif(N)
  k <- 0
  for (i in 2:N) {
    y <- rnorm(1, x[i-1], a)
    if (u[i] <= (dskewlap(y) / dskewlap(x[i-1])))
      x[i] <- y else {
        x[i] <- x[i-1]
        k <- k + 1
      }
  }
  return(list(x=x, k=k))
}
# function to generate the chain
r1 <- myrw.Metropolis(sigma[1], x0, N)
r2 <- myrw.Metropolis(sigma[2], x0, N)
r3 <- myrw.Metropolis(sigma[3], x0, N)
r4 <- myrw.Metropolis(sigma[4], x0, N)
# results by R
#r11 <-MetropolisC(sigma[1], x0, N)
#r21 <- MetropolisC(sigma[2], x0, N)
#r31 <- MetropolisC(sigma[3], x0, N)
#r41 <- MetropolisC(sigma[4], x0, N)
# results by Rcpp
c<-matrix(c(r1$k/N, r2$k/N, r3$k/N, r4$k/N),1,4)
colnames(c)<-c("sigma=0.05","sigma=0.5","sigma=2","sigma=16")
rownames(c)<-"rejection rate(R)"
pander::pandoc.table(c)
#b<-matrix(c(r11[N+1]/N, r21[N+1]/N, r31[N+1]/N, r41[N+1]/N),1,4)
#colnames(b)<-c("sigma=0.05","sigma=0.5","sigma=2","sigma=16")
#rownames(b)<-"rejection rate(Rcpp)"
#pander::pandoc.table(b)
```

As we can observe in the table, both the second and third chain have a rejection rate in the range [0.15, 0.5].

Then we will show what can happen to the random walk Metropolis sampler when the variance of proposal distribution change.

```{r}
#par(mfrow=c(2,2))
#plot(r11[1:N],type='l',ylab="X",xlab=expression(paste(sigma,'=0.05')))
# generated sample vs the time index when the variance of proposal distribution is 0.05
#plot(r21[1:N],type='l',ylab="X",xlab=expression(paste(sigma,'=0.5')))
# generated sample vs the time index when the variance of proposal distribution is 0.5
 #abline(h=-3,col='red',lwd=.3)
 #abline(h=3,col='red',lwd=.3)
 # convergence range
#plot(r31[1:N],type='l',ylab="X",xlab=expression(paste(sigma,'=2')))
# generated sample vs the time index when the variance of proposal distribution is 4
#abline(h=-3,col='red',lwd=.3)
#abline(h=3,col='red',lwd=.3)
# convergence range
#plot(r41[1:N],type='l',ylab="X",xlab=expression(paste(sigma,'=16')))
# generated sample vs the time index when the variance of proposal distribution is 16
#abline(h=-3,col='red',lwd=.3)
#abline(h=3,col='red',lwd=.3)# convergence range 

```

As is shown above, under the circumstances of proposal distribution with $N\left(X_{t}, 0.05^{2}\right)$, Chain 1 has not converged to the target in 2000 iterations. The chain in second($\sigma=0.5$) and third($\sigma=2$) plot converging to the target distribution while the former requires a little longer burn-in period. Finally, in the fourth plot, where $\sigma$= 16, most of the candidate points are rejected. The fourth chain converges, but it is inefficient.

* ### QQ plot

We want to observe the relationship between the generated samples by the two functions in a more intuitive way. So the QQ plot and histogram of the generated sample with $N\left(X_{t}, 0.5^{2}\right)$ and $N\left(X_{t}, 2^{2}\right)$ density superimposed are shown below respectively.

```{r}
#par(mfrow=c(2,2))
b <- 201 ; d<-101
#discard the burnin sample
y1 <- r2$x[b:N]
#y11<-r21[b:N]
y2 <- r3$x[d:N]
#y21<-r31[b:N]
a <- ppoints(100)
#QR <- qskewlap(1-a) 
# quantiles of Rayleigh
Q1 <- quantile(y1, a)
#Q11<-quantile(y11, a)
Q2 <- quantile(y2, a)
#Q21<-quantile(y21, a)
#qqplot(Q1, Q11, main="",xlab="R Sample Quantiles", ylab="Rcpp Sample Quantiles")
#hist(y11, breaks="scott", main="", xlab="", freq=FALSE)
#lines(QR, dskewlap(QR))
#qqplot(Q2, Q21, main="",xlab="R Sample Quantiles", ylab="Rcpp Sample Quantiles")
#hist(y21, breaks="scott", main="", xlab="", freq=FALSE)
#lines(QR, dskewlap(QR))
```

* ### Time Comparision

```{r}
#ts <- microbenchmark(MetropolisR=myrw.Metropolis(sigma[1], x0, N),
#                     MetropolisCpp=MetropolisC(sigma[1], x0, N))
#summary(ts)[,c(1,3,5,6)]
```

We can observe that compute efficiency enjoys a marvelous improvement using the Rcpp function compared with R function, which shows the great advantage of Rcpp function.
